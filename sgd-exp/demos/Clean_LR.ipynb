{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD Model with Linear regression which logs weights after half of iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from prettytable import PrettyTable\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numpy import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loading and preprocessing\n",
    "diabetes_data=pd.DataFrame(load_diabetes().data,columns=load_diabetes().feature_names)\n",
    "Y=load_diabetes().target\n",
    "X=load_diabetes().data\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardising\n",
    "scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test=scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 10)\n",
      "(442,)\n",
      "(309, 10)\n",
      "(133, 10)\n",
      "(309,)\n",
      "(133,)\n"
     ]
    }
   ],
   "source": [
    "#visualising\n",
    "diabetes_data.head(3)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.887249</td>\n",
       "      <td>1.049781</td>\n",
       "      <td>0.402089</td>\n",
       "      <td>1.210502</td>\n",
       "      <td>-0.077052</td>\n",
       "      <td>-0.020211</td>\n",
       "      <td>-0.617040</td>\n",
       "      <td>-0.011109</td>\n",
       "      <td>0.715123</td>\n",
       "      <td>0.205189</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.929383</td>\n",
       "      <td>-0.952579</td>\n",
       "      <td>-1.021087</td>\n",
       "      <td>-0.443706</td>\n",
       "      <td>0.042143</td>\n",
       "      <td>-0.312586</td>\n",
       "      <td>1.386315</td>\n",
       "      <td>-0.838291</td>\n",
       "      <td>-0.473037</td>\n",
       "      <td>0.474227</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.281705</td>\n",
       "      <td>1.049781</td>\n",
       "      <td>-0.433744</td>\n",
       "      <td>-0.084095</td>\n",
       "      <td>0.876507</td>\n",
       "      <td>1.190083</td>\n",
       "      <td>-0.154728</td>\n",
       "      <td>0.816072</td>\n",
       "      <td>-0.055962</td>\n",
       "      <td>-0.153529</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age       sex       bmi        bp        s1        s2        s3  \\\n",
       "0  0.887249  1.049781  0.402089  1.210502 -0.077052 -0.020211 -0.617040   \n",
       "1 -0.929383 -0.952579 -1.021087 -0.443706  0.042143 -0.312586  1.386315   \n",
       "2  0.281705  1.049781 -0.433744 -0.084095  0.876507  1.190083 -0.154728   \n",
       "\n",
       "         s4        s5        s6  Output  \n",
       "0 -0.011109  0.715123  0.205189   178.0  \n",
       "1 -0.838291 -0.473037  0.474227   111.0  \n",
       "2  0.816072 -0.055962 -0.153529   111.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pd.DataFrame(x_train,columns=load_diabetes().feature_names)\n",
    "train_data['Output']=y_train\n",
    "train_data.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "model_w = []\n",
    "model_b = []\n",
    "def CustomSGD(train_data, learning_rate, n_iter, k, divideby, init_w=None, init_b=0):\n",
    "    if init_w is None:\n",
    "        w = np.zeros(shape=(1, train_data.shape[1]-1))\n",
    "    else:\n",
    "        w = init_w\n",
    "    w_gradient = w\n",
    "    if init_b is None:\n",
    "        b = 0\n",
    "    else:\n",
    "        b_gradient = init_b\n",
    "    cur_iter = 1\n",
    "    while cur_iter <= n_iter: \n",
    "        temp = train_data.sample(k)\n",
    "        y = np.array(temp['Output'])\n",
    "        x = np.array(temp.drop('Output', axis=1))\n",
    "        w_gradient = np.zeros(shape=(1, train_data.shape[1]-1))\n",
    "        b_gradient = 0\n",
    "        for i in range(k):\n",
    "            prediction = np.dot(w, x[i]) + b\n",
    "            w_gradient = w_gradient + (-2) * x[i] * (y[i] - prediction)\n",
    "            b_gradient = b_gradient + (-2) * (y[i] - prediction)\n",
    "        w = w - learning_rate * (w_gradient / k)\n",
    "        b = b - learning_rate * (b_gradient / k)\n",
    "        \n",
    "        if cur_iter % 10 == 0:\n",
    "            train_loss = 0\n",
    "            for i in range(k):\n",
    "                train_loss = train_loss + ((y[i] - np.dot(w, x[i]) - b) ** 2)\n",
    "            print(\"Loss after\", cur_iter, \":\", train_loss, \"w:\", w, \"b:\", b)\n",
    "            if cur_iter > 0.5 * n_iter:\n",
    "                train_losses.append(train_loss)\n",
    "                model_w.append(w)\n",
    "                model_b.append(b)\n",
    "            cur_iter = cur_iter + 1\n",
    "            learning_rate = learning_rate / divideby    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict Function\n",
    "def predict(x,w,b):\n",
    "    y_pred=[]\n",
    "    for i in range(len(x)):\n",
    "        y=(np.dot(w,x[i])+b)\n",
    "        y=y.item()\n",
    "        y_pred.append(y)\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 10 : [198101.2439967] w: [[ 3.46372066  0.30662133 -1.62355887  2.10528917  3.39906675  1.99829706\n",
      "  -0.64442362  1.98559232  6.20652835  2.35187096]] b: [25.79705145]\n",
      "Loss after 20 : [63495.79577845] w: [[ 2.66073058  0.18034132  6.97204059  8.92795719  5.16264731  3.20536493\n",
      "  -2.89934236  6.73105737 10.98977091  4.7547053 ]] b: [48.81167099]\n",
      "Loss after 30 : [88664.77748147] w: [[ 0.11015977 -0.5930275  10.21576859 12.20499588  3.54515269  0.8637492\n",
      "  -0.50503168  4.09623373 10.24667278  3.11099738]] b: [67.44812594]\n",
      "Loss after 40 : [83747.30127694] w: [[ 1.73923282 -1.61938567 11.98345939 12.00465533  4.62555416  0.85274466\n",
      "  -2.00516628  4.97680629 15.04795233  3.49936136]] b: [81.54932525]\n",
      "Loss after 50 : [53716.4656356] w: [[ 1.73454109 -3.49327559 12.28407153 12.11572462  1.98544565 -1.05000793\n",
      "  -2.49973876  4.30164808 14.02583762  3.98528472]] b: [94.01482572]\n",
      "Loss after 60 : [27953.3988319] w: [[ 2.45226744 -4.32436254 15.96921375 14.86577303  3.55795433 -1.14190725\n",
      "  -2.67426314  4.41214703 18.76732484  3.8671565 ]] b: [104.79090155]\n",
      "Loss after 70 : [35176.38412461] w: [[ 1.73773386 -2.38558176 19.26868158 16.34862119  3.88498018 -0.07960383\n",
      "  -4.75284046  6.79233396 19.83432271  5.59265171]] b: [111.67103608]\n",
      "Loss after 80 : [26299.47644251] w: [[ 0.98003111 -5.0987288  20.5423478  17.19725987  1.65455583 -2.48729012\n",
      "  -4.36693598  5.08353994 19.2540036   4.79008487]] b: [117.44578471]\n",
      "Loss after 90 : [26793.66695466] w: [[ 2.23337658 -4.71930528 21.01822857 17.98139012  2.09439723 -2.44902537\n",
      "  -3.18989256  4.05635055 19.32400382  4.43053121]] b: [121.06181126]\n",
      "Loss after 100 : [24750.66424896] w: [[ 3.52401768 -4.73645574 22.80078252 18.05641194  2.45290422 -1.26729453\n",
      "  -3.77464904  4.12459293 18.75600754  3.67026151]] b: [124.82937891]\n",
      "Loss after 110 : [44588.10525918] w: [[ 3.15833806 -6.18823329 22.13210069 17.23128317  1.77424569 -1.76132757\n",
      "  -4.43884417  4.46938445 19.64045946  4.19663359]] b: [129.62948379]\n",
      "Loss after 120 : [35434.930384] w: [[ 6.07384713 -4.79266746 23.96343283 17.88152398 -1.2027087  -3.79651742\n",
      "  -5.3354901   4.21987063 18.59989861  6.00013635]] b: [133.95758815]\n",
      "Loss after 130 : [26406.52531836] w: [[ 5.80923796 -6.2532199  24.3920035  18.30609701 -2.55100217 -5.47216369\n",
      "  -5.21294814  3.11537716 18.8487979   5.88400571]] b: [137.3740185]\n",
      "Loss after 140 : [19461.09195826] w: [[ 4.18140895 -6.44817686 22.71872921 18.03705875 -2.20384313 -5.27541373\n",
      "  -5.68262072  3.90408303 19.65034373  4.26116113]] b: [140.3805691]\n",
      "Loss after 150 : [48760.09999438] w: [[ 1.8432418  -7.36376094 24.29136635 16.35927862 -1.64113291 -4.57711894\n",
      "  -5.79183784  4.38758869 19.95427521  5.20641752]] b: [142.1838566]\n",
      "Loss after 160 : [48300.70889905] w: [[ 2.5363429  -7.1308224  24.29452755 16.88720688 -0.1697412  -3.30264067\n",
      "  -5.98822817  5.64187037 21.27503238  5.764919  ]] b: [144.33774101]\n",
      "Loss after 170 : [21336.47923175] w: [[ 4.13329517 -6.07649047 24.35574036 17.34144187 -0.71437961 -3.5063825\n",
      "  -5.93753304  5.09190831 20.31555748  5.54076164]] b: [145.58516562]\n",
      "Loss after 180 : [26903.48674816] w: [[ 3.54982848 -5.46687868 24.8910703  17.6762048  -2.26889519 -4.64842972\n",
      "  -5.94889338  4.36283167 18.9078206   5.11323315]] b: [146.51102915]\n",
      "Loss after 190 : [47732.58737964] w: [[ 2.18922582 -6.84568538 26.16241093 17.62213976 -2.73398044 -5.47500742\n",
      "  -6.08026971  4.32134034 19.59603811  4.92700368]] b: [146.40606736]\n",
      "Loss after 200 : [13273.13536629] w: [[ 2.44877634 -6.53412671 25.21163432 16.3056888  -3.98027993 -6.75920459\n",
      "  -6.20079446  3.75124223 19.59310871  5.04002113]] b: [146.56138883]\n",
      "Loss after 210 : [30467.56116746] w: [[ 2.79795039 -7.56913962 26.44230792 17.10590557 -3.26970947 -6.16137133\n",
      "  -6.22036028  4.4707961  20.9154783   4.29577065]] b: [146.11785374]\n",
      "Loss after 220 : [20853.20835175] w: [[ 2.67350261 -7.14481598 26.003296   16.63605204 -4.01439852 -6.7478023\n",
      "  -7.48251428  5.17890614 21.521486    4.98171246]] b: [147.63205916]\n",
      "Loss after 230 : [31034.865117] w: [[ 2.96317812 -6.42182946 25.48629607 16.35639065 -2.53157291 -5.35955084\n",
      "  -6.71940524  5.40752197 21.86472091  6.01406474]] b: [147.6747067]\n",
      "Loss after 240 : [53582.15589723] w: [[ 1.32423657 -7.89135541 25.8521767  14.90405784 -1.31748649 -4.36653484\n",
      "  -5.59373236  4.84487938 21.86825168  6.50218091]] b: [149.32270032]\n",
      "Loss after 250 : [17772.40921789] w: [[ 0.90011876 -6.98501615 24.20113454 15.24901387 -1.42321707 -3.55931041\n",
      "  -6.31962859  5.42300226 20.55668121  5.09149992]] b: [150.87260904]\n",
      "Loss after 260 : [32133.11323211] w: [[ 1.51341281 -6.87737492 25.11496289 13.7608919  -0.10371971 -2.56110134\n",
      "  -5.49328728  5.89462391 20.97235086  5.8026241 ]] b: [149.0246631]\n",
      "Loss after 270 : [20003.71587489] w: [[ 2.42666994 -5.59492739 25.57574978 15.11188432  1.63091121 -1.28807711\n",
      "  -4.87766617  6.4383057  22.00271485  6.59472598]] b: [148.39832918]\n",
      "Loss after 280 : [16418.71753954] w: [[ 2.1153467  -5.32390306 25.03829618 14.22846785 -0.60125748 -3.23245511\n",
      "  -4.50842749  5.1839854  20.10343799  6.1952424 ]] b: [149.51592927]\n",
      "Loss after 290 : [40707.36985766] w: [[ 1.91777682 -7.22590362 25.37898109 13.5734073  -1.08823196 -4.20878172\n",
      "  -3.3766677   3.58735875 20.24375729  5.16887281]] b: [148.97259315]\n",
      "Loss after 300 : [29516.44530838] w: [[ 2.37968078 -7.59372561 24.47185557 13.00234711 -1.54270887 -4.2543089\n",
      "  -4.86594299  4.89556468 21.28320819  4.91663092]] b: [149.63156897]\n",
      "Loss after 310 : [32216.68347243] w: [[ 2.13482707e+00 -5.74958352e+00  2.55706930e+01  1.26125627e+01\n",
      "   1.15753624e-02 -2.38008171e+00 -5.35303435e+00  6.23061396e+00\n",
      "   2.23256877e+01  5.31640301e+00]] b: [148.9733913]\n",
      "Loss after 320 : [52005.49328732] w: [[ 2.38518973 -7.05323069 25.24436963 13.43772453  0.55569125 -1.73896029\n",
      "  -5.33529171  6.14912934 22.30582948  6.878627  ]] b: [150.1714108]\n",
      "Loss after 330 : [6243.76529386] w: [[ 1.76580846 -6.97989458 25.81086584 13.48325605 -0.18236286 -2.64795855\n",
      "  -4.91687473  6.29005275 22.10928357  6.57001462]] b: [150.60977328]\n",
      "Loss after 340 : [23795.69416398] w: [[ 1.4473152  -7.52204326 26.35119027 13.00414202 -0.59171869 -3.31037031\n",
      "  -4.19988927  5.27677214 21.67688454  7.21278025]] b: [150.01959148]\n",
      "Loss after 350 : [26564.11257913] w: [[ 3.36706273 -8.61957915 26.07621508 14.06805138  0.24517375 -2.8377914\n",
      "  -3.22232947  4.68934692 22.11770849  8.49028449]] b: [149.06962254]\n",
      "Loss after 360 : [24678.6465599] w: [[ 3.71507082 -7.74051189 25.20790718 13.20926014 -0.42482398 -3.38105524\n",
      "  -3.75669475  4.67433021 21.55706402  7.90677555]] b: [147.93761959]\n",
      "Loss after 370 : [30164.73826903] w: [[ 2.5742496  -9.01702869 26.98450587 13.03595489 -1.27599127 -3.27935734\n",
      "  -4.45137415  4.5557085  20.63591696  6.88986714]] b: [148.40051075]\n",
      "Loss after 380 : [18063.00069949] w: [[ 3.89927056 -8.82688841 27.32388474 13.63759967 -2.6135484  -4.27507088\n",
      "  -6.19235909  5.79683836 21.44516476  6.52548296]] b: [150.00202474]\n",
      "Loss after 390 : [45707.57931296] w: [[ 3.85958523 -9.79848568 27.1454979  11.90138153 -1.97313653 -3.04552992\n",
      "  -7.18301211  6.89079094 21.83532952  5.77031351]] b: [149.07040937]\n",
      "Loss after 400 : [30850.7786645] w: [[ 4.57594694 -7.56758448 28.04806077 12.03117423 -3.08699938 -3.84486098\n",
      "  -7.44785279  6.91147012 20.99008692  5.51304065]] b: [149.13520182]\n",
      "Loss after 410 : [36117.52815423] w: [[ 4.10822386 -6.22577213 27.32532944 11.49094352 -5.30872622 -7.02268968\n",
      "  -6.77969975  4.85902681 21.62628151  4.46388972]] b: [147.59238135]\n",
      "Loss after 420 : [10506.60649251] w: [[ 3.86740511 -7.58327012 27.14277747 12.33149337 -3.54218116 -6.14158859\n",
      "  -4.75730011  4.21857689 22.18046223  4.41228671]] b: [148.2163829]\n",
      "Loss after 430 : [56472.98281571] w: [[ 3.67697799 -8.59419769 26.92504465 10.90910156 -2.80789863 -5.14032223\n",
      "  -4.00240607  4.76903972 21.3289248   3.84521309]] b: [148.77005465]\n",
      "Loss after 440 : [33877.0437392] w: [[ 4.13951438 -7.04485422 25.97501902 10.18621229 -2.27573036 -4.9091172\n",
      "  -4.85405511  5.71492859 23.31477857  2.93436   ]] b: [147.23881904]\n",
      "Loss after 450 : [30905.6344352] w: [[ 3.30636919 -6.16874112 24.85887625  9.58855623 -1.29873049 -3.47152004\n",
      "  -5.53150989  7.01276136 22.1367262   3.26404222]] b: [146.60003162]\n",
      "Loss after 460 : [21879.50162546] w: [[ 3.95547584 -6.61613242 26.20302813  9.56216037 -4.06452243 -6.2226687\n",
      "  -6.51469154  6.15907605 23.28548586  4.09255311]] b: [149.30474911]\n",
      "Loss after 470 : [30094.7010105] w: [[ 3.83641612 -8.3162709  26.54543726  8.63705552 -4.60520943 -6.83295284\n",
      "  -6.58403965  5.97175217 22.94570764  3.03001653]] b: [149.90848238]\n",
      "Loss after 480 : [38987.62982958] w: [[ 5.32194799 -8.29341316 26.69633298 10.50510871 -2.54099432 -4.88776761\n",
      "  -6.53932289  6.96746055 23.52669153  3.61620928]] b: [150.04309331]\n",
      "Loss after 490 : [14879.38650055] w: [[ 5.97423053 -9.00240344 25.95771072 10.05397026 -3.40205578 -5.24019815\n",
      "  -7.46785393  7.02441472 23.27867441  3.48774336]] b: [149.81189743]\n",
      "Loss after 500 : [18705.35653163] w: [[ 6.25703737 -9.3546628  25.66248516 10.0379189  -2.59702536 -5.60731313\n",
      "  -6.69110799  7.14667377 25.34469822  2.97190821]] b: [148.59372927]\n",
      "Loss after 510 : [28217.58277835] w: [[ 6.57994818 -8.79872622 24.13637795 11.56181041 -1.43404847 -4.07690753\n",
      "  -7.38465593  8.76602603 25.42653496  2.27186846]] b: [148.02396913]\n",
      "Loss after 520 : [21484.1560152] w: [[  7.15999672 -10.42142166  24.80739362  12.09899701   0.35296324\n",
      "   -2.33065336  -6.39025644   8.77547835  25.37347221   4.2998226 ]] b: [147.91418136]\n",
      "Loss after 530 : [11335.22031424] w: [[  5.14412891 -12.00791957  24.46930889  11.15402859  -0.58036047\n",
      "   -2.71520446  -5.74693671   7.19085278  23.53975674   3.13018133]] b: [148.85694844]\n",
      "Loss after 540 : [14536.43165209] w: [[  4.34573906 -12.36717493  24.44524771  10.89457024  -0.13176542\n",
      "   -2.84857426  -3.83751581   5.57382176  22.77352396   2.67133313]] b: [149.47119565]\n",
      "Loss after 550 : [14886.13224214] w: [[  3.3344526  -13.37698286  26.20279165  10.56086855  -0.98379256\n",
      "   -3.29901167  -4.82380421   5.96548422  23.11231768   2.84036963]] b: [150.35793315]\n",
      "Loss after 560 : [33189.78288617] w: [[  1.90154121 -14.18792515  27.39850068  10.4912667   -2.85676957\n",
      "   -4.5361724   -5.68535562   5.36209106  22.1941785    3.95289049]] b: [150.92219478]\n",
      "Loss after 570 : [28978.74778891] w: [[  2.47251836 -14.0404051   28.70830779  12.03678369  -2.01351344\n",
      "   -4.84868232  -3.91752611   4.17841724  23.87709818   5.14501375]] b: [150.18371503]\n",
      "Loss after 580 : [11315.98556131] w: [[  3.57115212 -12.19595221  28.5340316   13.29735829  -2.93343452\n",
      "   -5.34079681  -4.30101797   4.2802611   23.67155667   5.07516101]] b: [150.05199749]\n",
      "Loss after 590 : [60208.42693181] w: [[  2.83147682 -12.48960401  29.56907804  13.38151197  -1.57291452\n",
      "   -4.05542393  -3.78065108   4.54077208  24.26914562   5.26178429]] b: [150.41594428]\n",
      "Loss after 600 : [30088.6615569] w: [[  3.09415703 -12.11317091  27.97658346  13.72589255  -1.12748635\n",
      "   -4.13077114  -3.75464767   4.36868716  25.01906143   4.4266107 ]] b: [149.84986595]\n",
      "Loss after 610 : [13346.34275214] w: [[  3.09807548 -10.68002821  27.1778718   15.45169718  -0.80808949\n",
      "   -3.11030309  -2.99866755   3.98194755  24.02098421   5.0811107 ]] b: [149.9525631]\n",
      "Loss after 620 : [21599.71313465] w: [[  4.58875089 -10.3717679   25.62745387  14.98662841  -1.81705511\n",
      "   -4.18563311  -2.49170317   3.12582411  23.24606963   5.7273508 ]] b: [150.18945595]\n",
      "Loss after 630 : [10001.25562522] w: [[  4.03894363 -10.67149209  24.77891252  17.00588394  -2.86639052\n",
      "   -5.06920359  -3.28646261   3.14549056  23.22848574   4.92682053]] b: [149.33245699]\n",
      "Loss after 640 : [15809.39544285] w: [[ 3.77142405 -9.35491982 24.61619684 16.9965167  -2.3209968  -3.96893845\n",
      "  -4.58557234  4.80177277 23.45997696  6.09104063]] b: [148.60569632]\n",
      "Loss after 650 : [17540.55265131] w: [[  4.52427376 -10.58396387  23.04561477  15.77692396  -0.95934657\n",
      "   -2.62305674  -3.75482973   4.41015886  22.80120245   4.56896123]] b: [149.35453727]\n",
      "Loss after 660 : [26090.19701511] w: [[  3.23861232 -10.50147439  23.1178998   13.90646206  -1.24562252\n",
      "   -2.19199659  -5.55132708   5.7677091   22.8464783    4.62660025]] b: [149.89889937]\n",
      "Loss after 670 : [18864.63586534] w: [[  3.39139176 -10.11576452  21.77130365  13.43067358  -2.66302049\n",
      "   -3.06665608  -6.39212609   5.82255178  22.34459991   4.05375452]] b: [149.10664349]\n",
      "Loss after 680 : [11046.53190094] w: [[  3.1733009  -10.31638838  22.44546997  12.8008575   -1.62571233\n",
      "   -1.66327006  -6.28376208   6.08737158  22.01595488   4.52804766]] b: [148.67497053]\n",
      "Loss after 690 : [9330.47317862] w: [[  2.30444012 -11.93760161  22.72764174  10.80491324  -3.17128543\n",
      "   -2.90861041  -7.06057376   5.97327497  22.0463491    3.1386727 ]] b: [148.17906652]\n",
      "Loss after 700 : [50703.09510252] w: [[  2.11095625 -12.29709434  22.24125796  13.71349403  -2.57465257\n",
      "   -1.33035147  -6.99569109   6.24221298  21.13999317   2.92610413]] b: [149.86847013]\n",
      "Loss after 710 : [42937.22938224] w: [[  4.22425825 -10.87097246  23.37019719  15.25665632  -1.93586989\n",
      "    0.07250752  -7.75203558   7.00859026  20.44275658   3.80412555]] b: [148.99228727]\n",
      "Loss after 720 : [13635.76256831] w: [[ 3.54239436 -8.86732963 22.00339322 14.87275803 -2.24146045  0.35836936\n",
      "  -8.58569156  7.3132182  19.72761001  3.47536343]] b: [149.5967952]\n",
      "Loss after 730 : [45826.2870245] w: [[ 5.15892521 -9.89182741 22.18540694 14.49981772 -4.0537562  -1.16880384\n",
      "  -8.12421903  5.8807878  18.13662532  4.67473493]] b: [150.41993269]\n",
      "Loss after 740 : [11914.2656479] w: [[ 7.20833233 -8.97957588 21.98828557 14.91182291 -4.61798477 -1.66255602\n",
      "  -8.50077916  6.05354654 18.75364929  5.08249762]] b: [149.85189139]\n",
      "Loss after 750 : [50595.76190577] w: [[ 5.92770219 -9.38466277 23.24318846 14.65780433 -4.33500295 -2.52378476\n",
      "  -7.1688546   5.66967293 20.19883891  4.84591843]] b: [150.12516557]\n",
      "Loss after 760 : [22721.28833424] w: [[  5.22460138 -11.72144508  22.58990768  13.9103651   -5.33287327\n",
      "   -3.55536441  -6.7336645    4.66860558  19.72173058   4.03404027]] b: [149.31263904]\n",
      "Loss after 770 : [13335.59817975] w: [[  6.03570899 -11.66303417  23.9348734   15.18826366  -5.161052\n",
      "   -3.80036273  -7.28871025   4.99395582  21.98035225   5.41167806]] b: [150.54842114]\n",
      "Loss after 780 : [22564.09102805] w: [[  5.05433555 -12.97452397  23.65368981  14.22752248  -6.36842667\n",
      "   -4.52230415  -8.14060593   5.19738408  21.68512653   3.79589072]] b: [151.90577683]\n",
      "Loss after 790 : [21220.12422383] w: [[  5.77936299 -13.131317    23.91230398  13.87139608  -6.04984378\n",
      "   -4.47208786  -7.84913137   4.95279179  21.51267704   4.24357585]] b: [150.15290323]\n",
      "Loss after 800 : [33793.9919923] w: [[  4.00287717 -11.88274096  23.07334066  12.76197943  -4.9531154\n",
      "   -2.91386245  -7.22712521   5.13248316  20.93322795   4.91549814]] b: [149.39120784]\n",
      "Loss after 810 : [23419.41121969] w: [[  2.67644289 -11.83403623  23.09795258  11.95864294  -5.52348088\n",
      "   -2.61987424  -7.57029762   5.59429821  19.03599354   3.70124477]] b: [149.16264211]\n",
      "Loss after 820 : [10380.10433839] w: [[  2.28120733 -10.91794803  23.71486657  11.59619891  -5.6276992\n",
      "   -2.3328182   -8.77051294   6.58114874  19.47865256   2.74247414]] b: [150.39456655]\n",
      "Loss after 830 : [19413.19780878] w: [[  2.06298352 -11.91957202  25.58198256  13.06216117  -4.11565732\n",
      "   -1.37891286  -7.93701984   6.58039391  21.49139873   4.78986842]] b: [151.31741597]\n",
      "Loss after 840 : [28314.87273549] w: [[  2.84989281 -10.56255622  24.35772543  13.91513632  -5.27158876\n",
      "   -2.51602494  -7.88734118   5.8869981   20.93521916   6.0580518 ]] b: [150.66445573]\n",
      "Loss after 850 : [29372.40943109] w: [[  1.95293768 -11.32767964  24.84342963  14.13018237  -4.98270489\n",
      "   -2.45196523  -8.1626635    6.41017432  21.79804702   5.48009033]] b: [149.98345283]\n",
      "Loss after 860 : [23281.5955648] w: [[  2.220846   -10.97219154  25.42938376  14.56516465  -6.58100685\n",
      "   -4.03860197  -7.67178773   4.69319643  20.56193709   5.60507265]] b: [150.35851994]\n",
      "Loss after 870 : [33751.87339798] w: [[  3.81755859 -10.94724656  26.17884059  12.92709473  -6.08981662\n",
      "   -4.08060814  -6.78827698   4.34456773  21.47355037   6.77286083]] b: [149.70794891]\n",
      "Loss after 880 : [21595.59431363] w: [[  3.95192549 -11.36019729  25.45145072  11.47456458  -6.47593924\n",
      "   -4.00012524  -7.04565218   4.73174135  20.68101818   6.62497564]] b: [149.19378846]\n",
      "Loss after 890 : [21129.7363586] w: [[  3.51386966 -10.09435458  26.44751052  11.10393372  -5.25072445\n",
      "   -1.88467945  -6.49658927   5.35131113  19.1957353    7.0910195 ]] b: [147.53333245]\n",
      "Loss after 900 : [22423.81731688] w: [[  3.68477305 -10.7399103   27.09308065  11.00581727  -5.95529017\n",
      "   -2.29685071  -7.07569517   5.5397579   19.49577611   6.07305369]] b: [146.98967628]\n",
      "Loss after 910 : [24766.42477219] w: [[  3.03046075 -10.98057919  27.90056427  10.40730373  -6.6163161\n",
      "   -2.81197335  -7.83321337   5.78257665  19.34446447   5.77961947]] b: [148.03618438]\n",
      "Loss after 920 : [34471.90621455] w: [[  3.11068004 -10.27580093  28.00710642  11.72895987  -6.42773418\n",
      "   -2.7777889   -6.59562225   4.45112723  18.94228108   5.99221448]] b: [148.69236883]\n",
      "Loss after 930 : [13335.62903605] w: [[  3.10120924 -12.60574727  27.56350489  10.96859225  -5.70796179\n",
      "   -2.24467965  -5.23138264   3.89603254  18.82884523   5.2983632 ]] b: [149.27338376]\n",
      "Loss after 940 : [29466.11207436] w: [[  3.79952449 -10.89245918  27.74998825  10.89807295  -4.87948911\n",
      "   -1.16781271  -6.15453505   5.53647546  19.61110838   6.57993271]] b: [148.91483109]\n",
      "Loss after 950 : [26840.64325383] w: [[  4.32373367 -10.35594135  27.03459946  12.4029827   -4.7329453\n",
      "   -0.44795464  -5.88276535   5.86677605  19.26400884   6.25116492]] b: [148.84518131]\n",
      "Loss after 960 : [38854.84969466] w: [[ 5.54396958 -9.88102252 26.65518196 15.3121563  -5.28920159 -0.87090668\n",
      "  -6.85611444  6.49412335 19.99997213  7.28966639]] b: [150.39153052]\n",
      "Loss after 970 : [31704.21692661] w: [[  5.89021098 -10.76843689  26.91636318  16.05917156  -3.62484275\n",
      "   -0.2637784   -5.39859492   5.92314863  21.37293845   7.10540355]] b: [149.5613112]\n",
      "Loss after 980 : [43960.45973791] w: [[  5.12442215 -10.79015202  27.15290485  14.36861153  -4.13003723\n",
      "    0.05606337  -4.35589134   4.80220413  18.7418282    6.08888642]] b: [148.12011448]\n",
      "Loss after 990 : [18923.56902675] w: [[  5.18611065 -10.18640056  27.29705579  14.17203922  -4.99477079\n",
      "   -0.32465558  -4.27860883   4.60034689  18.36010023   7.40823131]] b: [148.33156652]\n",
      "Loss after 1000 : [38522.18358681] w: [[ 4.6849599  -7.55359535 26.79535097 15.42748335 -4.7041983   0.84856513\n",
      "  -5.76952465  5.91896418 18.52742916  7.67478075]] b: [148.03541539]\n",
      "[[ 4.6849599  -7.55359535 26.79535097 15.42748335 -4.7041983   0.84856513\n",
      "  -5.76952465  5.91896418 18.52742916  7.67478075]] [148.03541539]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error : 3042.2495511881316\n"
     ]
    }
   ],
   "source": [
    "w,b = CustomSGD(train_data, learning_rate=0.01, n_iter=1000, k=10, divideby=1, init_w=None, init_b=None)\n",
    "print(w, b)\n",
    "\n",
    "y_pred_customsgd=predict(x_test,w,b)\n",
    "\n",
    "plt.scatter(y_test,y_pred_customsgd)\n",
    "plt.grid()\n",
    "plt.xlabel('Actual y')\n",
    "plt.ylabel('Predicted y')\n",
    "plt.title('Scatter plot from actual y and predicted y')\n",
    "plt.show()\n",
    "print('Mean Squared Error :',mean_squared_error(y_test, y_pred_customsgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (1000, 2, 1) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[0;32m      3\u001b[0m pca \u001b[38;5;241m=\u001b[39m PCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m X_pca \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Plot the data in the reduced-dimensional space after PCA\u001b[39;00m\n\u001b[0;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    279\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1349\u001b[0m     )\n\u001b[0;32m   1350\u001b[0m ):\n\u001b[1;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:454\u001b[0m, in \u001b[0;36mPCA.fit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    433\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 454\u001b[0m     U, S, Vt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    455\u001b[0m     U \u001b[38;5;241m=\u001b[39m U[:, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components_]\n\u001b[0;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhiten:\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:483\u001b[0m, in \u001b[0;36mPCA._fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_array_api_compliant:\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPCA with svd_solver=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marpack\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not supported for Array API inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    481\u001b[0m     )\n\u001b[1;32m--> 483\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;66;03m# Handle n_components==None\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:951\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    949\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    950\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 951\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    954\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    955\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ben_b\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 3 dimensions. The detected shape was (1000, 2, 1) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(model_weights)\n",
    "\n",
    "# Plot the data in the reduced-dimensional space after PCA\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c='red', marker='o', edgecolors='k', alpha=0.8)\n",
    "plt.title('Data after PCA (2D Projection)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
