{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "from data.handler import load_and_transform_data, get_data_loader\n",
    "from training.train_funcs import train_clean_model, single_epoch\n",
    "from vizualization.tensors import imshow\n",
    "\n",
    "from devinterp.optim.sgld import SGLD\n",
    "from devinterp.slt.llc import estimate_learning_coeff_with_summary\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "plt.rcParams[\"figure.figsize\"]=15,12  # note: this cell may need to be re-run after creating a plot to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging face stores downloads at ~/.cache/huggingface/datasets by default \n",
    "\n",
    "dataset_name = 'cifar10'\n",
    "batch_size = 32\n",
    "cache_dir = os.getenv(\"CACHE_DIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_and_transform_data(dataset_name, 'train', augment=False, download_dir=cache_dir)\n",
    "test_dataset = load_and_transform_data(dataset_name, 'test', augment=False, download_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = get_data_loader(train_dataset, batch_size, shuffle=True)\n",
    "test_dataloader = get_data_loader(test_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=False).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "checkpoints = []\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = single_epoch(model, \"train\", criterion, optimizer, train_dataloader, device)\n",
    "    test_loss = single_epoch(model, \"val\", criterion, optimizer, test_dataloader, device)\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    checkpoints += [copy.deepcopy(model)]\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test loss\n",
    "\n",
    "epochs = list(range(n_epochs))\n",
    "plt.plot(epochs, train_losses, label='Train')\n",
    "plt.plot(epochs, test_losses, label='Test')\n",
    "plt.xlabel('Training epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and test loss for MNIST model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILONS = [1e-5, 1e-4, 1e-3]\n",
    "GAMMAS = [1, 10, 100]\n",
    "NUM_CHAINS = 8\n",
    "NUM_DRAWS = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_llcs_sweeper(model, epsilons, gammas, device):\n",
    "    results = {}\n",
    "    for epsilon in epsilons:\n",
    "        for gamma in gammas:\n",
    "            optim_kwargs = dict(\n",
    "                lr=epsilon,\n",
    "                noise_level=1.0,\n",
    "                elasticity=gamma,\n",
    "                num_samples=50000, # Hard coded because len(train_data) is a little hard with the huggingface stuff.\n",
    "                temperature=\"adaptive\",\n",
    "            )\n",
    "            pair = (epsilon, gamma)\n",
    "            results[pair] = estimate_learning_coeff_with_summary(\n",
    "                model=model,\n",
    "                loader=train_dataloader,\n",
    "                criterion=criterion,\n",
    "                sampling_method=SGLD,\n",
    "                optimizer_kwargs=optim_kwargs,\n",
    "                num_chains=NUM_CHAINS,\n",
    "                num_draws=NUM_DRAWS,\n",
    "                device=device,\n",
    "                online=True,\n",
    "            )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimate_llcs_sweeper(checkpoints[-1], EPSILONS, GAMMAS, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mars",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
